---
title: "Polynomial optimzation analysis"
author: "Andrew Lampinen"
output: html_document
---

```{r}
library(tidyverse)
library(Hmisc)
library(boot)
```

# data loading

```{r}
parent_dir = "optimization_results"
subdirs = c("updated", "untrained_baseline", "tcnh")
num_runs = 5
```

```{r}
read_config = function(config_file) { 
  config = read_delim(config_file, delim="\n") %>%
    separate(`key, value`, c("key", "value"), sep=",", extra="merge") %>%
    spread(key, value) %>%
    mutate_at(c("base_train_tasks", "base_eval_tasks", "meta_class_train_tasks", "meta_class_eval_tasks", "meta_map_train_tasks", "meta_map_eval_tasks"), function(x) {
      x = gsub("\\\"|[][]| |\'", "", x)
      return(str_split(x, ","))
    } )
}
```

```{r}
load_d = function(results_dir, result_subdirs, num_runs, file_type) {
  d = data.frame()
  for (run_i in 0:(num_runs-1)) {
    for (result_subdir in result_subdirs) {
      filename = sprintf("%s/%s/run%i_%s.csv", results_dir, result_subdir, run_i, file_type)
      print(filename)
      if (!file.exists(filename)) {
        print(paste("skipping ", filename, sep=""))
        next
      }
      if (grepl("config", file_type)) {
        this_d = read_config(filename)
      } else {
        this_d = read.csv(filename, check.names=F, header=T) 
        names(this_d) <- make.unique(names(this_d))

      }
      this_d = this_d %>%
        mutate(run = run_i,
               run_type = result_subdir)
      d = d %>%
        bind_rows(this_d)
    }
    
  }
  return(d)
}
```

```{r}
config_d = load_d(parent_dir, subdirs, num_runs, "run_config")
guess_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "guess_opt_losses") %>% 
  mutate(init_type = "guess")
random_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "random_init_opt_losses") %>% 
  mutate(init_type = "random_init")
arbitrary_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "arbitrary_trained_opt_losses") %>% 
  mutate(init_type = "arbitrary_train_task")
centroid_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "trained_centroid_opt_losses") %>% 
  mutate(init_type = "trained_centroid")

loss_d = bind_rows(guess_opt_loss_d, random_opt_loss_d, arbitrary_opt_loss_d, centroid_opt_loss_d)
guess_opt_loss_d = data.frame()
random_opt_loss_d = data.frame()
arbitrary_opt_loss_d = data.frame()
centroid_opt_loss_d = data.frame()
```

# some manipulation

```{r}
loss_d = loss_d %>%
  gather(task_and_train_or_eval, loss, -epoch, -run, -run_type, -init_type) %>%
  separate(task_and_train_or_eval, c("task", "train_or_eval"), sep=":") %>%
  mutate(train_or_eval = sub("\\.[0-9]+", "", train_or_eval))
  
```


# basic plots
```{r}
theme_set(theme_classic())
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval"),
       aes(x=epoch, y=log(loss), color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean") +
  facet_wrap(run_type + run ~ .)
```
```{r}
loss_d = loss_d %>%
  filter(run_type != "untrained_baseline" | init_type == "arbitrary_train_task") %>%
  mutate(init_type = ifelse(run_type == "untrained_baseline", "untrained", init_type))
```

```{r}
chance = loss_d %>% filter(init_type == "untrained", 
                           run_type == "untrained_baseline",
                           train_or_eval == "eval",
                           epoch == 0) %>%
  summarise(loss = mean(loss, na.rm=T)) %>%
  pull(loss)
chance
```


```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                run_type != "tcnh") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.4) +
  geom_hline(yintercept=log(chance), alpha=0.5, linetype=2) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  annotate("text", x=240, y=log(chance) + 1, alpha=0.5, label="Chance") +
  labs(x="Epoch (training task embeddings on new data)", y="Loss on new tasks") +
  guides(color=guide_legend(title="")) +
  scale_y_log10(breaks=c(1e-02, 1e-01, 1, 1e1, 1e2), labels = c(0.01, 0.1, 1, 10, 100))

#ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_curves.png", width=6, height=4)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                run_type != "tcnh",
                !is.na(loss),
                init_type != "untrained") %>%
         group_by(init_type, run, task) %>%
         summarise(regret_int=sum(loss)) %>%
         group_by(init_type, run) %>%
         summarise(mean_lri=mean(regret_int, na.rm=T)) %>%
         ungroup() %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid\nof tasks", "Arbitrary\ntrain task", "Random\nvector", "Untrained model"))),
       aes(x=init_type, y=mean_lri, fill=init_type)) +
  geom_bar(stat="summary",
           fun.y="mean") +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.5) +
  scale_fill_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  labs(x="Initialization", y="Mean cumulative regret on new task") +
  guides(fill=F)

#ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_cumulative_regret.png", width=6, height=4)
```
```{r}
intermediate_data = loss_d %>%
  filter(train_or_eval == "eval",
         run_type != "tcnh",
         !is.na(loss),
         init_type != "untrained") %>%
  group_by(init_type, run, task) %>%
  summarise(regret_int=sum(loss)) %>%
  group_by(init_type, run) %>%
  summarise(mean_lri=mean(regret_int, na.rm=T)) 

intermediate_data %>%
  group_by(init_type) %>%
  summarise(mean_lri=mean(mean_lri))
```

```{r}
set.seed(0)  # reproducibility
CI_data = intermediate_data %>%
  group_by(init_type) %>%
  do(result=boot.ci(boot(., function(x, inds) {return(mean(x[inds,]$mean_lri))}, R=5000))) %>%
  mutate(CI_low=result$percent[4], CI_high=result$percent[5])
CI_data
```

## MM paper plots

```{r}
logified_chance = loss_d %>% 
  filter(init_type == "untrained", 
         run_type == "untrained_baseline",
         train_or_eval == "eval",
         epoch == 0) %>%
  filter(!is.na(loss),
         loss > 1e-6) %>%  # avoid a few log-splosions
  mutate(log_loss=log10(loss)) %>%
  summarise(loss = 10^mean(log10(loss), na.rm=T)) %>%
  pull(loss)
logified_chance
```

```{r}
ggplot(loss_d %>%
         filter(run_type != "tcnh",
                train_or_eval == "eval") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.2) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_hline(yintercept=logified_chance, alpha=0.5, linetype=2) +
  scale_color_manual(values=c("#762a83", "#a6dba0", "#6abe71", "#3a9e51", "#505050")) +
#  scale_linetype_manual(values=c("solid", "11", "11", "11", "11")) +
  annotate("text", x=240, y=logified_chance - 1.1, alpha=0.5, label="Chance") +
  labs(x="Epoch (training task representations on new data)", y="Average log-loss on new tasks") +
  guides(color=guide_legend(title=NULL))+#, linetype=guide_legend(title=NULL)) +
  scale_y_log10(breaks=c(1e-02, 1e-01, 1, 1e1, 1e2), labels = c(0.01, 0.1, 1, 10, 100)) +
  theme(legend.position=c(0.75, 0.885),
        legend.key.height = unit(0.66, 'lines'),
        legend.key.width = unit(1, 'lines'))

#ggsave("../metamapping_paper/figures/optimization_curves.png", width=4, height=3)
ggsave("../metamapping_paper/figures/optimization_curves.pdf", width=4, height=3)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                run_type != "tcnh", 
                !is.na(loss),
                init_type != "untrained") %>%
         group_by(init_type, run, task) %>%
         summarise(regret_int=sum(loss)) %>%
         group_by(init_type, run) %>%
         summarise(mean_lri=mean(regret_int, na.rm=T)) %>%
         ungroup() %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid\nof tasks", "Arbitrary\ntrain task", "Random\nvector", "Untrained model"))),
       aes(x=init_type, y=mean_lri, fill=init_type)) +
  geom_bar(stat="summary",
           fun.y="mean") +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.5) +
  scale_fill_manual(values=c("#762a83", "#a6dba0", "#6abe71", "#3a9e51", "#505050")) +
#  scale_fill_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  labs(x="Task representation initialization", y="Mean cumulative loss on new task") +
  guides(fill=F)

ggsave("../metamapping_paper/figures/optimization_regret.png", width=4, height=3)
```

# Hyper vs TCNH

```{r}
g = ggplot(loss_d %>%
             filter(train_or_eval == "eval",
                    run_type != "untrained",
                    init_type %in% c("guess", "trained_centroid")) %>%
             mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid of\ntrained tasks", "Arbitrary train task", "Random vector", "Untrained model")),
                  run_type = factor(run_type,
                                    levels=c("updated", "tcnh"),
                                    labels=c("HyperNetwork\narchitecture",
                                             "Task concatenated\narchitecture"))
           ),
       aes(x=epoch, y=loss, color=run_type, linetype=init_type)) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_line(aes(group=interaction(run, run_type, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.2,
            show.legend=F) +
  geom_hline(yintercept=logified_chance, alpha=0.5, linetype=2) +
  scale_color_manual(values=c("#e41a1c", "#841010")) +
  labs(x="Epoch (training task representations on new data)", y="Average log-loss on new tasks") +
  guides(color=guide_legend(title=NULL),
         linetype=guide_legend(title=NULL)) +
  scale_y_log10(breaks=c(1e-02, 1e-01, 1, 1e1, 1e2), labels = c(0.01, 0.1, 1, 10, 100)) +
  theme(legend.key.width = unit(1,"cm"),
        legend.key.height = unit(0.75,"cm"))

g  +
  annotate("text", x=240, y=logified_chance + 1, alpha=0.5, label="Chance")
#ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_tcnh_curves.png", width=6, height=4)

g +
  annotate("text", x=60, y=logified_chance + 1, alpha=0.5, label="Chance") +
  theme(legend.position=c(0.8, 0.8),
        legend.key.size = unit(0.3, 'lines'),
        legend.margin = margin(0, 0, 0, 0),
        legend.spacing = unit(0, 'lines'))

ggsave("../metamapping_paper/figures/polynomial_optimization_tcnh_curves.png", width=4, height=3)
```

```{r}
g = ggplot(loss_d %>%
             filter(train_or_eval == "eval",
                    run_type != "untrained",
                    !is.na(loss),
                    init_type %in% c("guess", "trained_centroid")) %>%
             mutate(run_type = factor(run_type,
                                      levels=c("updated", "tcnh"),
                                      labels=c("HyperNetwork\narchitecture",
                                               "Task concatenated\narchitecture"))) %>%
             group_by(init_type, run_type, run, task) %>%
             summarise(regret_int=sum(loss)) %>%
             group_by(init_type, run_type, run) %>%
             summarise(mean_lri=mean(regret_int, na.rm=T)) %>%
             ungroup() %>%
             mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid\nof tasks", "Arbitrary\ntrain task", "Random\nvector", "Untrained model"))),
           aes(x=init_type, y=mean_lri, fill=run_type)) +
  geom_bar(stat="summary",
           fun.y="mean",
           position="dodge") +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.5,
                position=position_dodge(0.9)) +
  scale_fill_manual(values=c("#e41a1c", "#841010")) +
  labs(x="Initialization", y="Mean cumulative regret on new task") +
  guides(fill=guide_legend(title=NULL))

g + 
  theme(legend.key.size = unit(2, "lines"))
ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_tcnh_cumulative_regret.png", width=6, height=4)


g +
  theme(legend.position=c(0.3, 0.8),
        legend.key.size = unit(1.5, 'lines'))

ggsave("../metamapping_paper/figures/polynomial_optimization_tcnh_cumulative_regret.png", width=4, height=3)

```

```{r}
intermediate_data = loss_d %>%
  filter(train_or_eval == "eval",
         run_type != "untrained",
         !is.na(loss),
         init_type %in% c("guess", "trained_centroid")) %>%
  group_by(init_type, run_type, run, task) %>%
  summarise(regret_int=sum(loss)) %>%
  group_by(init_type, run_type, run) %>%
  summarise(mean_lri=mean(regret_int, na.rm=T)) 

intermediate_data %>%
  group_by(init_type, run_type) %>%
  summarise(mean_lri=mean(mean_lri))
```

```{r}
set.seed(0)  # reproducibility
CI_data = intermediate_data %>%
  group_by(init_type, run_type) %>%
  do(result=boot.ci(boot(., function(x, inds) {return(mean(x[inds,]$mean_lri))}, R=5000))) %>%
  mutate(CI_low=result$percent[4], CI_high=result$percent[5])
CI_data
```