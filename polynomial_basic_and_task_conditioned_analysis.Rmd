---
title: "Polynomial task-conditioned task-net analysis"
author: "Andrew Lampinen"
output: html_document
---

```{r}
library(tidyverse)
```

# data loading

```{r}
parent_dir = "conditioned_vs_hyper_results"
subdirs = c("polynomials_results",
            "polynomials_results_non_hyper",
            "metaclass_lesion",
            "nonhomoiconic"
            )
num_runs = 5
```

```{r}
read_config = function(config_file) { 
  config = read_delim(config_file, delim="\n") %>%
    separate(`key, value`, c("key", "value"), sep=",", extra="merge") %>%
    spread(key, value) %>%
    mutate_at(c("base_train_tasks", "base_eval_tasks", "meta_class_train_tasks", "meta_class_eval_tasks", "meta_map_train_tasks", "meta_map_eval_tasks"), function(x) {
      x = gsub("\\\"|[][]| |\'", "", x)
      return(str_split(x, ","))
    } )
}
```

```{r}
load_d = function(results_dir, result_subdirs, num_runs, file_type) {
  d = data.frame()
  for (run_i in 0:(num_runs-1)) {
    for (result_subdir in result_subdirs) {
      filename = sprintf("%s/%s/run%i_%s.csv", results_dir, result_subdir, run_i, file_type)
      print(filename)
      if (!file.exists(filename)) {
        print(paste("skipping ", filename, sep=""))
        next
      }
      if (grepl("config", file_type)) {
        this_d = read_config(filename)
      } else {
        this_d = read.csv(filename, check.names=F, header=T) 
        names(this_d) <- make.unique(names(this_d))

      }
      this_d = this_d %>%
        mutate(run = run_i,
               run_type = result_subdir)
      d = d %>%
        bind_rows(this_d)
    }
    
  }
  return(d)
}
```

```{r}
config_d = load_d(parent_dir, subdirs, num_runs, "run_config")
loss_d = load_d(parent_dir, subdirs, num_runs, "losses")
meta_true_d = load_d(parent_dir, subdirs, num_runs, "meta_true_losses")
meta_baseline_d = load_d(parent_dir, subdirs, num_runs, "meta_true_baselines")
```

# some manipulation

```{r}
loss_d = loss_d %>%
  filter(epoch %% 50 == 0) %>%
  gather(task_and_train_or_eval, loss, -epoch, -run, -run_type) %>%
  separate(task_and_train_or_eval, c("task", "train_or_eval"), sep=":") %>%
  mutate(train_or_eval = sub("\\.[0-9]+", "", train_or_eval),
         meta = grepl("is|permute|add|mult|square", task)) %>%
  filter(!is.na(loss))
  
```

```{r}
meta_true_d = meta_true_d %>%
  filter(epoch %% 200 == 0) %>%
  gather(task, loss, -epoch, -run, -run_type) %>%
  separate(task, c("meta_task", "mapping_toe", "base_task_toe", "source", "target"), ":|->") %>%
  filter(!is.na(loss))
  
```

```{r}
meta_summarized_baselines = meta_baseline_d %>%
  group_by(run) %>%
  summarize(zeros_loss = mean(zeros_loss),
            unadapted_loss = mean(unadapted_loss)) %>%
  ungroup() %>%
  summarize(zeros_loss = mean(zeros_loss),
            unadapted_loss = mean(unadapted_loss)) 
meta_summarized_baselines

chance_baseline = meta_summarized_baselines$zeros_loss[1]
unadapted_baseline = meta_summarized_baselines$unadapted_loss[1]
```


# basic plots
```{r}
theme_set(theme_classic())
```

```{r}
ggplot(loss_d %>% 
         filter(epoch > 1000),
       aes(x=epoch, y=loss, color=train_or_eval)) +
  geom_line(stat="summary",
            fun.y="mean") +
  facet_grid(meta ~ run_type, scales="free")
```

```{r}
ggplot(meta_true_d,
       aes(x=epoch, y=loss, color=base_task_toe, linetype=mapping_toe)) +
  geom_line(stat="summary",
            fun.y="mean") +
  facet_grid(run ~run_type )
```

```{r}
ggplot(meta_true_d,
       aes(x=epoch, y=loss, color=base_task_toe, linetype=mapping_toe)) +
  geom_line(stat="summary",
            fun.y="mean") +
  geom_hline(yintercept=chance_baseline, alpha=0.5, linetype=2) +
  geom_hline(yintercept=unadapted_baseline, alpha=0.5, linetype=3) +
  annotate("text", x=3800, y=15.7, label="Outputting zeros", alpha=0.5) +
  annotate("text", x=4000, y=13.1, label="No adaptation", alpha=0.5) +
  xlim(NA, 5000) +
  facet_wrap(~run_type) 
  #facet_grid(run ~run_type )
```

```{r}
meta_true_d %>%
  group_by(run_type, run, mapping_toe, base_task_toe) %>%
  filter(epoch == max(epoch),
         base_task_toe == "example_is_eval") %>%
  summarize(mean_loss = mean(loss, na.rm=T))
```



```{r}
ggplot(loss_d %>% filter(!meta),
       aes(x=epoch, y=loss, linetype=train_or_eval, color=train_or_eval)) +
  geom_line(stat="summary",
            fun.y="mean") +
  facet_wrap(~ run_type)
```


# MM paper plot

summary by run
```{r}
meta_summarized_baselines = meta_baseline_d %>%
  group_by(run, base_task_toe, mapping_toe) %>%
  summarize(zeros_loss = mean(zeros_loss),
            unadapted_loss = mean(unadapted_loss)) %>%
  ungroup() %>%
  mutate_if(is.factor, function(x) {
    return(gsub(" ", "", as.character(x)))
  })
```


```{r}
summarized_d = meta_true_d %>%
  filter(run_type == "polynomials_results") %>%
  group_by(run) %>%
  filter(epoch == max(epoch)) %>%
  group_by(run, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}
ggplot(summarized_d %>% 
         filter(base_task_toe == "eval"),
       aes(x=mapping_toe, y=variance_explained_vs_zeros,
           color=mapping_toe)) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.25,
                size=1) +
  geom_point(stat="summary",
             fun.y="mean",
             size=3) +
  geom_spoke(aes(x=mapping_toe, y=unadapted_variance_explained_vs_zeros, angle=0, radius=1),
             stat="summary",
             fun.y="mean",
             linetype=3,# alpha=0.5, 
             size=1,
             position=position_nudge(x=-0.5)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  annotate("text", x=2.16, y=1.05, label="Optimal adaptation", alpha=0.5) +
  geom_text(data=summarized_d %>%
              filter(base_task_toe == "eval") %>%
              group_by(mapping_toe) %>%
              summarize(text_pos = mean(unadapted_variance_explained_vs_zeros) + 0.06),
            aes(y=text_pos), label="No adaptation", alpha=0.5, position=position_nudge(x=0.165)) +
  scale_y_continuous(limits = c(0., NA), breaks = c(0., 0.5, 1.), labels = c("0%", "50%", "100%")) +
  scale_color_manual(values=c("#984ea3", "#ff7f00")) +
  guides(color=F) +
  labs(x = "Meta-mapping trained or new",
       y = "Evaluation performance (%)")
  

#ggsave("../metamapping_paper/figures/polynomials_adaptation.png", width=4, height=3)
ggsave("../../psych/dissertation/2-HoMM/figures/polynomials_adaptation.png", width=5, height=3)
```

```{r}
summarized_d %>%
  filter(base_task_toe == "eval") %>%
  group_by(mapping_toe) %>%
  do(results1=mean_cl_boot(.$variance_explained_vs_zeros),
     results2=mean_cl_boot(.$unadapted_variance_explained_vs_zeros)) %>%
  summarize(performance=results1$y,
            performance95min=results1$ymin,
            performance95max=results1$ymax,
            unadapted=results2$y,
            unadapted95min=results2$ymin,
            unadapted95max=results2$ymax,
            )
```

```{r}
meta_base_rep_similarities = data.frame(run=0:4, meta_base_rep_similarities=c(0.24416769, 0.10534607, 0.14665513, 0.07097830, 0.21243110))
```

```{r}
summarized_d = summarized_d %>%
  left_join(meta_base_rep_similarities)
```

```{r}
ggplot(summarized_d %>% 
         filter(base_task_toe == "eval"),
       aes(x=meta_base_rep_similarities, y=variance_explained_vs_zeros,
           color=mapping_toe)) +
  geom_point()
```


# conditioned vs hyper plot


```{r}
cvh_summarized_d = meta_true_d %>%
  filter(grepl("polynomials_results", run_type)) %>%
  group_by(run, run_type) %>%
  filter(epoch == max(epoch)) %>%
  group_by(run, run_type, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}
ggplot(cvh_summarized_d %>% 
         filter(base_task_toe == "eval") %>%
         mutate(run_type = factor(run_type,
                                  levels=c("polynomials_results", "polynomials_results_non_hyper"),
                                  labels=c("HyperNetwork\narchitecture",
                                           "Task concatenated\narchitecture"))),
       aes(x=mapping_toe, y=variance_explained_vs_zeros,
           color=run_type)) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.25,
                size=1,
                position=position_dodge(width=0.4)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=3,
             position=position_dodge(width=0.4)) +
  # geom_spoke(aes(x=mapping_toe, y=unadapted_variance_explained_vs_zeros, angle=0, radius=1),
  #            stat="summary",
  #            fun.y="mean",
  #            linetype=3,# alpha=0.5, 
  #            size=1,
  #            position=position_nudge(x=-0.5)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  annotate("text", x=2.16, y=1.01, label="Optimal adaptation", alpha=0.5) +
  # geom_text(data=cvh_summarized_d %>%
  #             filter(base_task_toe == "eval") %>%
  #             group_by(mapping_toe, run_type) %>%
  #             summarize(text_pos = mean(unadapted_variance_explained_vs_zeros) + 0.06),
  #           aes(y=text_pos, color=), label="No adaptation", alpha=0.5, position=position_nudge(x=0.165)) +
  scale_y_continuous(limits = c(0.8, NA), breaks = c(0.8, 0.9, 1.), labels = c("80%", "90%", "100%")) +
scale_color_manual(values=c("#e41a1c", "#841010")) +
  guides(color=F)+#guide_legend(title=NULL)) +
  labs(x = "Meta-mapping trained or held-out",
       y = "Evaluation performance (%)") +
  theme(legend.position=c(0.8, 0.75))


ggsave("../metamapping_paper/figures/conditioned_vs_hyper_polynomials.png", width=4, height=3)
```

```{r}
cvh_summarized_d %>%
  filter(base_task_toe == "eval") %>%
  group_by(mapping_toe, run_type) %>%
  do(results1=mean_cl_boot(.$variance_explained_vs_zeros),
     results2=mean_cl_boot(.$unadapted_variance_explained_vs_zeros)) %>%
  mutate(performance=results1$y,
            performance95min=results1$ymin,
            performance95max=results1$ymax,
            unadapted=results2$y,
            unadapted95min=results2$ymin,
            unadapted95max=results2$ymax,
            ) %>%
  select(-starts_with("results"))
```

# breakdown by type
```{r}
meta_mm_summarized_baselines = meta_baseline_d %>%
  mutate(meta_task_type = str_extract(meta_mapping, "square|add|mult|perm")) %>%
  group_by(run, meta_task_type, base_task_toe, mapping_toe) %>%
  summarize(zeros_loss = mean(zeros_loss),
            unadapted_loss = mean(unadapted_loss)) %>%
  ungroup() %>%
  mutate_if(is.factor, function(x) {
    return(gsub(" ", "", as.character(x)))
  })
```

```{r}
mm_summarized_d = meta_true_d %>%
  filter(run_type == "polynomials_results") %>%
  mutate(meta_task_type = str_extract(meta_task, "square|add|mult|perm")) %>%
  group_by(run) %>%
  filter(epoch == max(epoch),
         !is.na(loss)) %>%
  group_by(run, meta_task_type, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss), num=n()) %>%
  ungroup() %>%
  inner_join(meta_mm_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  gather(result_type, value, contains("variance_explained_vs_zeros")) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained", "New")),
         result_type = factor(result_type, levels=c("variance_explained_vs_zeros", "unadapted_variance_explained_vs_zeros"),
                              labels=c("HoMM", "No adaptation\ncontrol")),
         )
```

```{r}
ggplot(mm_summarized_d %>%
         filter(base_task_toe == "eval"),
       aes(x=meta_task_type, y=value,
           color=mapping_toe,# linetype=result_type, 
#           alpha=result_type,
           shape=result_type)) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.25,
                size=1,
                position=position_dodge(0.5)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=2,
             position=position_dodge(0.5)) +
  # geom_spoke(aes(y=unadapted_variance_explained_vs_zeros, angle=0, radius=0.95),
  #            stat="summary",
  #            fun.y="mean",
  #            linetype=3,# alpha=0.5, 
  #            size=1,
  #            position=position_nudge(x=-0.5)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  geom_hline(yintercept=0., alpha=0.5,
             linetype=3) +
  annotate(geom="text",
            x=3.67, y=1.06, color="black",
            label="Optimal adaptation", alpha=0.5) +
  # geom_text(data=mm_summarized_d %>%
  #             filter(base_task_toe == "eval") %>%
  #             group_by(mapping_toe, meta_task_type) %>%
  #             summarize(text_pos = mean(unadapted_variance_explained_vs_zeros) + 0.06),
  #           aes(y=text_pos), label="No adaptation", alpha=0.5, position=position_nudge(x=0.165)) +
  scale_y_continuous(limits = c(NA, 1.08), breaks = c(0., 0.5, 1.), labels = c("0%", "50%", "100%")) +
  scale_color_manual(values=c("#984ea3", "#ff7f00")) +
#  scale_alpha_manual(values=c(1., 0.5)) +
#  scale_linetype_manual(values=c(1,3)) +
  guides(color=guide_legend(title="Meta-mapping"),
         #linetype=guide_legend(title="Result type"),
         shape=guide_legend(title="Result type")) +
  labs(x = "Meta-mapping type",
       y = "Evaluation performance (%)")
  

ggsave("../metamapping_paper/figures/polynomials_adaptation_by_mapping.png", width=5, height=3)
ggsave("../../psych/dissertation/2-HoMM/figures/polynomials_adaptation_by_mapping.png", width=6, height=4)

```

# metaclass lesion  

```{r}
mcl_summarized_d = meta_true_d %>%
  filter(run_type %in% c("polynomials_results", "metaclass_lesion")) %>%
  group_by(run, run_type) %>%
  filter(epoch == max(epoch)) %>%
  group_by(run, run_type, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}
ggplot(mcl_summarized_d %>% 
         filter(base_task_toe == "eval") %>%
         mutate(run_type = factor(run_type,
                                  levels=c("polynomials_results", "metaclass_lesion"),
                                  labels=c("HoMM",
                                           "HoMM with no\nmeta-classification"))),
       aes(x=mapping_toe, y=variance_explained_vs_zeros,
           color=run_type)) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.25,
                size=1,
                position=position_dodge(width=0.4)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=3,
             position=position_dodge(width=0.4)) +
  # geom_spoke(aes(x=mapping_toe, y=unadapted_variance_explained_vs_zeros, angle=0, radius=1),
  #            stat="summary",
  #            fun.y="mean",
  #            linetype=3,# alpha=0.5, 
  #            size=1,
  #            position=position_nudge(x=-0.5)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  annotate("text", x=2.16, y=1.01, label="Optimal adaptation", alpha=0.5) +
  # geom_text(data=mcl_summarized_d %>%
  #             filter(base_task_toe == "eval") %>%
  #             group_by(mapping_toe, run_type) %>%
  #             summarize(text_pos = mean(unadapted_variance_explained_vs_zeros) + 0.06),
  #           aes(y=text_pos, color=), label="No adaptation", alpha=0.5, position=position_nudge(x=0.165)) +
  scale_y_continuous(limits = c(0.8, NA), breaks = c(0.8, 0.9, 1.), labels = c("80%", "90%", "100%")) +
  scale_color_manual(values=c("#e41a1c", "#fcbba1")) +
  
  guides(color=guide_legend(title=NULL)) +
  labs(x = "Meta-mapping trained or held-out",
       y = "Evaluation performance (%)") +
  theme(legend.position=c(0.8, 0.75))


ggsave("../metamapping_paper/figures/metaclass_lesion_polynomials.png", width=4, height=3)
ggsave("../../psych/dissertation/2-HoMM/figures/metaclass_lesion_polynomials.png", width=5, height=3)
```

```{r}
set.seed(0)  # reproducibility
mcl_summarized_d %>%
  filter(base_task_toe == "eval") %>%
  group_by(run_type, mapping_toe) %>%
  do(results=mean_cl_boot(.$variance_explained_vs_zeros, na.rm=T)) %>%
  mutate(mean=results$y,
         CIlow=results$ymin,
         CIhigh=results$ymax)
```

```{r}
t.test(variance_explained_vs_zeros ~ run_type, mcl_summarized_d %>%
         filter(base_task_toe == "eval",
                mapping_toe == "Trained\nmeta-mapping"), paired=T)
```

```{r}
t.test(variance_explained_vs_zeros ~ run_type, mcl_summarized_d %>%
         filter(base_task_toe == "eval",
                mapping_toe == "New\nmeta-mapping"), paired=T)
```

## meta-learning performance
```{r}
summarized_loss_d = loss_d %>%
  filter(run_type %in% c("polynomials_results", "polynomials_results_non_hyper"),
         !meta) %>%  
  group_by(run_type, run, task, train_or_eval) %>%
  mutate(initial_loss = head(loss, n=1)) %>%
  ungroup() %>%
  group_by(run_type,run, epoch, train_or_eval) %>%
  summarize(loss = mean(loss, na.rm=T),
            initial_loss = mean(initial_loss, na.rm=T)) %>%
  ungroup() %>%
  mutate(variance_explained_vs_initial = (initial_loss - loss) / initial_loss)
```


```{r}
ggplot(summarized_loss_d %>%
         filter(run_type == "polynomials_results"),
       aes(x=epoch, y=variance_explained_vs_initial, color=train_or_eval)) +
  geom_line(aes(group=interaction(run, train_or_eval)),
            alpha=0.2) +
  geom_line(stat="summary",
            fun.y="mean",
            size=1) +
  scale_color_manual(breaks=c("train", "eval"), 
                     labels=c("Trained\npolynomials", "Held-out\npolynomials"),
                     values=c("#4d9221", "#c51b7d")) +
  labs(y="Normalized meta-learning performance", x="Epoch") +
  guides(color=guide_legend(title=NULL)) +
  theme(legend.position=c(0.8, 0.25),
        legend.key.size = unit(1.5, "lines"))

ggsave("../metamapping_paper/figures/basic_meta_learning_polynomials.png", width=4, height=3)
#ggsave("../../psych/dissertation/2-HoMM/figures/basic_meta_learning_polynomials.png", width=5, height=3)
```

```{r}
set.seed(0)  # reproducibility
summarized_loss_d %>%
  filter(epoch %in% c(0, 5000)) %>%
  group_by(run_type, train_or_eval, epoch) %>%
  do(results=mean_cl_boot(.$variance_explained_vs_initial, na.rm=T)) %>%
  mutate(mean=results$y,
         CIlow=results$ymin,
         CIhigh=results$ymax)
```

# nonhomoiconic

```{r}
nh_summarized_d = meta_true_d %>%
  filter(run_type %in% c("polynomials_results", "nonhomoiconic")) %>%
  group_by(run, run_type) %>%
  filter(epoch == max(epoch)) %>%
  group_by(run, run_type, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}
ggplot(nh_summarized_d %>% 
         filter(base_task_toe == "eval") %>%
         mutate(run_type = factor(run_type,
                                  levels=c("polynomials_results", "nonhomoiconic"),
                                  labels=c("HoMM",
                                           "Non-homoiconic MM"))),
       aes(x=mapping_toe, y=variance_explained_vs_zeros,
           color=run_type)) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.25,
                size=1,
                position=position_dodge(width=0.4)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=3,
             position=position_dodge(width=0.4)) +
  # geom_spoke(aes(x=mapping_toe, y=unadapted_variance_explained_vs_zeros, angle=0, radius=1),
  #            stat="summary",
  #            fun.y="mean",
  #            linetype=3,# alpha=0.5, 
  #            size=1,
  #            position=position_nudge(x=-0.5)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  annotate("text", x=2.16, y=1.01, label="Optimal adaptation", alpha=0.5) +
  # geom_text(data=nh_summarized_d %>%
  #             filter(base_task_toe == "eval") %>%
  #             group_by(mapping_toe, run_type) %>%
  #             summarize(text_pos = mean(unadapted_variance_explained_vs_zeros) + 0.06),
  #           aes(y=text_pos, color=), label="No adaptation", alpha=0.5, position=position_nudge(x=0.165)) +
  scale_y_continuous(limits = c(0., 1.), breaks = c(0.8, 0.9, 1.), labels = c("80%", "90%", "100%")) +
  scale_color_manual(values=c("#e41a1c", "#000000")) +
  
  guides(color=guide_legend(title=NULL)) +
  labs(x = "Meta-mapping trained or held-out",
       y = "Evaluation performance (%)") +
  theme(legend.position=c(0.8, 0.75))


ggsave("../metamapping_paper/figures/nonhomoiconic_polynomials.png", width=4, height=3)
ggsave("../../psych/dissertation/2-HoMM/figures/nonhomoiconic_polynomials.png", width=5, height=3)
```

```{r}
set.seed(0)  # reproducibility
nh_summarized_d %>%
  filter(base_task_toe == "eval") %>%
  group_by(run_type, mapping_toe) %>%
  do(results=mean_cl_boot(.$variance_explained_vs_zeros, na.rm=T)) %>%
  mutate(mean=results$y,
         CIlow=results$ymin,
         CIhigh=results$ymax)
```

## plot combining main results + nonhomoiconic

```{r}
ggplot(nh_summarized_d %>% 
         filter(base_task_toe == "eval") %>%
         mutate(run_type = factor(run_type,
                                  levels=c("polynomials_results", "nonhomoiconic"),
                                  labels=c("HoMM",
                                           "Non-hom.\nMM"))),
       aes(x=mapping_toe, y=variance_explained_vs_zeros,
           color=run_type)) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.4,
                size=1,
                position=position_dodge(width=0.4)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=3,
             position=position_dodge(width=0.4)) +
  # geom_spoke(aes(x=mapping_toe, y=unadapted_variance_explained_vs_zeros, angle=0, radius=1),
  #            stat="summary",
  #            fun.y="mean",
  #            linetype=3,# alpha=0.5, 
  #            size=1,
  #            position=position_nudge(x=-0.5)) +
  geom_spoke(aes(x=mapping_toe, y=unadapted_variance_explained_vs_zeros, angle=0, radius=1),
             stat="summary",
             fun.y="mean",
             linetype=3,# alpha=0.5, 
             size=1,
             color="#000000",
             position=position_nudge(x=-0.5)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  annotate("text", x=1.9, y=1.05, label="Optimal adaptation", alpha=0.5) +
  geom_text(data=nh_summarized_d %>%
              filter(base_task_toe == "eval",
                     run_type == "polynomials_results") %>%
              group_by(mapping_toe, run_type) %>%
              summarize(text_pos = mean(unadapted_variance_explained_vs_zeros) - 0.06) %>%
              mutate(run_type = factor(run_type,
                                       levels=c("polynomials_results", "nonhomoiconic"),
                                       labels=c("HoMM",
                                                "Non-hom.\nMM"))),
            aes(y=text_pos), label="No adaptation", alpha=0.5,# position=position_nudge(x=0.165),
            color="#000000",
            show.legend=F) +
  scale_y_continuous( breaks = c(0., 0.5, 1.), labels = c("0%", "50%", "100%")) +
  scale_color_manual(values=c("#e41a1c", "#000000")) +
  
  guides(color=guide_legend(title=NULL)) +
  labs(x = "Meta-mapping trained or held-out",
       y = "Evaluation performance (%)") #+
#  theme(legend.position=c(0.25, 0.5),
#        legend.title=element_blank(),
#        legend.background = element_blank(),
#        legend.box.background = element_rect(colour = "black"))



ggsave("../metamapping_paper/figures/polynomials_adaptation_results_with_nonhomm.png", width=4, height=3)
```


## homoiconic vs. nonhomiconic breakdown by type

```{r}
nh_less_summarized_d = meta_true_d %>%
  filter(run_type %in% c("polynomials_results", "nonhomoiconic")) %>%
  mutate(meta_task_type = str_extract(meta_task, "square|add|mult|perm")) %>%
  group_by(run, run_type) %>%
  filter(epoch == max(epoch)) %>%
  group_by(run, run_type, meta_task_type, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_mm_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}

ggplot(nh_less_summarized_d %>% 
         filter(base_task_toe == "eval") %>%
         mutate(run_type = factor(run_type,
                                  levels=c("polynomials_results", "nonhomoiconic"),
                                  labels=c("HoMM",
                                           "Non-hom.\nMM"))),
       aes(x=meta_task_type, y=variance_explained_vs_zeros,
           color=run_type)) +
  facet_wrap(~mapping_toe) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.4,
                size=1,
                position=position_dodge(width=0.4)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=3,
             position=position_dodge(width=0.4)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  scale_y_continuous(limits=c(0, 1), breaks = c(0., 0.5, 1.), labels = c("0%", "50%", "100%")) +
  scale_color_manual(values=c("#e41a1c", "#000000")) +
  
  guides(color=guide_legend(title=NULL)) +
  labs(x = "Meta-mapping trained or held-out",
       y = "Evaluation performance (%)") #+



ggsave("../metamapping_paper/figures/polynomials_adaptation_results_with_nonhomm_by_mm_type.png", width=6, height=3)
```


# varying meta-batch sizes

## data loading

```{r}
vmbs_parent_dir = "."
vmbs_subdirs = c("polynomial_mapped_rep_results")
vmbs_num_runs = 4
```

```{r}
vmbs_training_loss_d = load_d(vmbs_parent_dir, vmbs_subdirs, vmbs_num_runs, "losses")
vmbs_loss_d = load_d(vmbs_parent_dir, vmbs_subdirs, vmbs_num_runs, "varied_mbs_losses")
vmbs_meta_true_d = load_d(vmbs_parent_dir, vmbs_subdirs, vmbs_num_runs, "varied_mbs_meta_true_losses")
```

baseline (epoch 0):
```{r}
vmbs_initial_loss_d = vmbs_training_loss_d %>%
  filter(epoch == 0) %>%
  select(-epoch) %>%
  gather(task_and_train_or_eval, loss, -run, -run_type) %>%
  separate(task_and_train_or_eval, c("task", "train_or_eval"), sep=":") %>%
  mutate(train_or_eval = sub("\\.[0-9]+", "", train_or_eval),
         meta = grepl("is|permute|add|mult|square", task)) %>%
  filter(!is.na(loss)) %>%
  group_by(run_type,run, train_or_eval) %>%
  summarize(initial_loss = mean(loss, na.rm=T)) %>%
  ungroup()
  
```


```{r}
vmbs_loss_d = vmbs_loss_d %>%
  gather(task_and_train_or_eval, loss, -meta_batch_size, -run, -run_type) %>%
  separate(task_and_train_or_eval, c("task", "train_or_eval"), sep=":") %>%
  mutate(train_or_eval = sub("\\.[0-9]+", "", train_or_eval),
         meta = grepl("is|permute|add|mult|square", task)) %>%
  filter(!is.na(loss))
  
```

```{r}
vmbs_meta_true_d = vmbs_meta_true_d %>%
  gather(task, loss, -meta_batch_size, -run, -run_type) %>%
  separate(task, c("meta_task", "mapping_toe", "base_task_toe", "source", "target"), ":|->") %>%
  filter(!is.na(loss))
  
```

## base tasks

```{r}
vmbs_summarized_loss_d = vmbs_loss_d %>%
  group_by(run_type, run, meta_batch_size, train_or_eval) %>%
  do(CI=mean_cl_boot(.$loss, na.rm=T)) %>%
  mutate(loss = CI$y, loss_lower=CI$ymin, loss_upper=CI$ymax) %>%
  ungroup() %>%
  left_join(vmbs_initial_loss_d) %>%
  mutate(variance_explained_vs_initial = (initial_loss - loss) / initial_loss,
         variance_explained_vs_initial_lower = (initial_loss - loss_lower) / initial_loss,
         variance_explained_vs_initial_upper = (initial_loss - loss_upper) / initial_loss)
```


```{r}
ggplot(vmbs_summarized_loss_d,
       aes(x=meta_batch_size, y=variance_explained_vs_initial, color=train_or_eval, fill=train_or_eval)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  annotate("text", x=13.5, y=0.333, angle=90, size=3, label="Polynomial space dimension", alpha=0.5) +
  geom_segment(data=data.frame(train_or_eval=c("")), inherit_aes=F,
               aes(color=NULL, fill=NULL),
               x=15, xend=15, y=0, yend=1, alpha=0.5, linetype=3) +
  geom_ribbon(stat="summary",
              fun.data="mean_cl_boot",
              alpha=0.25,
              colour=NA,
              show.legend=F) +
  geom_line(stat="summary",
            fun.y="mean",
            size=1) +
  # geom_line(aes(group=interaction(run, train_or_eval)),
  #           alpha=0.2) +
  scale_color_manual(breaks=c("train", "eval"), 
                     labels=c("Trained\npolynomials", "Held-out\npolynomials"),
                     values=c("#4d9221", "#c51b7d")) +
  labs(y="Normalized base task performance", x="Number of examples") +
  guides(color=guide_legend(title=NULL)) +
  scale_y_continuous(limits=c(0, 1), breaks=c(0, 0.5, 1), labels=c("0%", "50%", "100%")) +
  scale_x_log10(breaks=c(1, 2, 4, 8, 16, 32, 50)) +
  theme(legend.position=c(0.8, 0.25),
        legend.key.size = unit(1.5, "lines"))

ggsave("../metamapping_paper/figures/varied_mbs_base_polynomials.png", width=4, height=3)
```

## meta true


```{r}
vmbs_summarized_d = vmbs_meta_true_d %>%
  filter(!grepl("square", meta_task)) %>% # square performance is weird, so restricting to others for the aggregated plot
  group_by(run, meta_batch_size, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}
ggplot(vmbs_summarized_d %>% 
         filter(base_task_toe == "eval"),
       aes(x=meta_batch_size, y=variance_explained_vs_zeros,
           color=mapping_toe, fill=mapping_toe)) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  geom_ribbon(stat="summary",
              fun.data="mean_cl_boot",
              alpha=0.25,
              colour=NA,
              show.legend=F) +
  geom_line(stat="summary",
            fun.y="mean",
            size=1) +
  annotate("text", x=6, y=1.05, label="Optimal adaptation", alpha=0.5) +
  scale_y_continuous(breaks = c(0., 0.5, 1.), labels = c("0%", "50%", "100%")) +
  scale_x_continuous(breaks=c(1, 2, 4, 8, 16, 32, 50)) +
  coord_cartesian(ylim=c(0, NA)) + 
  scale_color_manual(values=c("#984ea3", "#ff7f00")) +
  scale_fill_manual(values=c("#984ea3", "#ff7f00")) +
  guides(color=guide_legend(title=NULL)) +
  labs(x = "Number of examples",
       y = "Average evaluation performance (%)") + 
  theme(legend.position=c(0.8, 0.25),
        legend.key.size = unit(1.5, "lines"))
  

ggsave("../metamapping_paper/figures/varied_mbs_meta_true_polynomials.png", width=4, height=3)
```



```{r}
vmbs_less_summarized_d = vmbs_meta_true_d %>%
  mutate(meta_task_type = str_extract(meta_task, "square|add|mult|perm")) %>%
  group_by(run, meta_batch_size, meta_task_type, mapping_toe, base_task_toe) %>%
  summarise(mean_loss = mean(loss, na.rm=T)) %>%
  ungroup() %>%
  inner_join(meta_mm_summarized_baselines) %>%
  mutate_at(vars(contains("toe")), function(x) str_extract(x, "train|eval")) %>%
  mutate(variance_explained_vs_zeros = (zeros_loss - mean_loss) / zeros_loss,
         unadapted_variance_explained_vs_zeros = (zeros_loss - unadapted_loss) / zeros_loss) %>%
  mutate(mapping_toe = factor(mapping_toe, levels=c("train", "eval"),
                              labels=c("Trained\nmeta-mapping", "New\nmeta-mapping")))
```

```{r}
ggplot(vmbs_less_summarized_d %>% 
         filter(base_task_toe == "eval"),
       aes(x=meta_batch_size, y=variance_explained_vs_zeros,
           color=mapping_toe, fill=mapping_toe)) +
  facet_wrap(~meta_task_type) +
  geom_hline(yintercept=1., alpha=0.5,
             linetype=2) +
  geom_ribbon(stat="summary",
              fun.data="mean_cl_boot",
              alpha=0.25,
              colour=NA,
              show.legend=F) +
  geom_line(stat="summary",
            fun.y="mean",
            size=1) +
  annotate("text", x=12, y=1.05, label="Optimal adaptation", alpha=0.5) +
  scale_y_continuous(breaks = c(0., 0.5, 1.), labels = c("0%", "50%", "100%")) +
  scale_x_continuous(breaks=c(1, 2, 4, 8, 16, 32, 50)) +
  coord_cartesian(ylim=c(0, NA)) + 
  scale_color_manual(values=c("#984ea3", "#ff7f00")) +
  scale_fill_manual(values=c("#984ea3", "#ff7f00")) +
  guides(color=guide_legend(title=NULL)) +
  labs(x = "Number of examples",
       y = "Average evaluation performance (%)")+ 
  theme(legend.position=c(0.8, 0.25),
        legend.key.size = unit(1.5, "lines"))
  

ggsave("../metamapping_paper/figures/varied_mbs_meta_true_by_mm_polynomials.png", width=8, height=6)
```



